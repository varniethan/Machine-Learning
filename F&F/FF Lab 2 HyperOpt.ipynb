{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F&F Lab 2 — HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this?\n",
    "\n",
    "_HyperOpt_ is an abbreviation for _Hyperparameter optimisation_, which is exactly what it sounds like: it's the process of automatically tuning hyperparameters to get the best performance from your model.\n",
    "\n",
    "Defining the difference between hyperparameters and normal parameters can be somewhat tricky (in the fully general sense; it's usually straight forward for any given model). In fact, if you Google for the difference many people define hyperparameters as the part of the algorithm you set manually and the parameters as the part set by the data; this clearly contradicts the existence of _HyperOpt_, and is a good lesson in why you should avoid websites like _towardsdatascience.com_ (it's more wrong than right). However, this does capture it in some sense: parameters are set by the data and hyperparameters are set by something else. In the case of _HyperOpt_ you use a _validation_ set for learning the hyperparameters; it needs to be a separate data set because hyperparameters can almost always be set such that overfitting occurs. You will then need a third _test_ set to determine the final, true, performance of the complete model because, in tuning the model to fit the _validation_ set, you risk overfitting to the _validation_ set via the hyperparameters! The rule is that, as soon as a data set influences the model then measuring performance with it is no longer valid, though may still be useful. You may extrapolate that to what it means for a researcher to work on the same data set for any length of time, and hence realise that in practise following this rule exactly is unreasonable.\n",
    "\n",
    "The HyperOpt is going to be done using a genetic algorithm; just follow the slides and do whatever (it will probably work!). Part of the point here is to get a feel for the options, so try out whatever seems like a reasonable idea and compare your approach to your neighbours: see whose approach can reliably find a good solution in the least time. In addition, you're going to implement a ML algorithm which you probably haven't seen before \"_Random Kitchen Sinks_\". This is because it's an example of the idea that, under the right circumstances, you can actually replace optimisation with randomisation, which is useful in some situations and hence worth introducing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features = 11\n",
      "train examples = 8192\n",
      "validation examples = 8192\n",
      "test examples = 8192\n"
     ]
    }
   ],
   "source": [
    "for name in ['train', 'validation', 'test']:\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    with open(name + '.csv', newline='') as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        first = True\n",
    "        for row in reader:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "\n",
    "            x.append([float(v) for v in row[:-1]])\n",
    "            y.append(float(row[-1]))\n",
    "\n",
    "    x = numpy.array(x)\n",
    "    y = numpy.array(y)\n",
    "    \n",
    "    # If you haven't seen this before this is a techneque to write variables by name,\n",
    "    # where that name is a string constructed at runtime — it's a bad idea almost always\n",
    "    # and only hapenning here because Jupyter is also a bad idea...\n",
    "    globals()[name + '_x'] = x\n",
    "    globals()[name + '_y'] = y\n",
    "\n",
    "    \n",
    "print(f'features = {train_x.shape[1]}')\n",
    "print(f'train examples = {train_x.shape[0]}')\n",
    "print(f'validation examples = {validation_x.shape[0]}')\n",
    "print(f'test examples = {test_x.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "You always want a baseline performance measure, so you know what bad looks like! In this case, what happens if you always predict that y is the mean of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test root mean square error = 3.3468054970123737\n"
     ]
    }
   ],
   "source": [
    "mean_y = train_y.mean()\n",
    "\n",
    "mean_rmse = numpy.sqrt(numpy.square(test_y - mean_y).mean())\n",
    "print(f'Baseline test root mean square error = {mean_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Kitchen Sinks\n",
    "\n",
    "Consider a neural network with a single hidden layer:\n",
    "$$\\text{input} \\rightarrow \\text{hidden} \\rightarrow \\text{output}$$\n",
    "\n",
    "There are two blocks of weights, specifically the first block connecting the input to the hidden layer and then the second block connecting the hidden layer to the output layer; traditionally all weights would be fitted to the training set using back propagation combined with stochastic gradient descent and an optimiser such as adam.\n",
    "\n",
    "It happens to be the case that instead of doing this you may just randomise the first block of weights instead; to understand why you need to dive into random basis functions, which is probably a bit much for now, but if you really want to then you can see the seqeuence of papers that built up to this idea on one of the [authors websites](https://people.eecs.berkeley.edu/~brecht/kitchensinks.html). The useful consequences of doing this are\n",
    "\n",
    "* The optimisation of just the last layer is linear regression: the problem is now convex and much easier to solve. No backpropagation!\n",
    "* The choice of non-linearity is no longer limited by the requirement that it can be optimised with gradient descent; periodic non-linearities (e.g. sine) are often choosen because they keep changing through the space and hence work better. When sine is choosen it's an example of something called _random Fourier features_.\n",
    "* You don't have to store the weights of the first layer; instead you can store the seed that a psuedo random number generator (pRNG) uses to generate them and do so every time you need them. There is no reason to do that here as storage isn't tight on a modern computer; more a concern for low power devices or saving disk space.\n",
    "* They are a universal function approximator, i.e. can represent any function as the width of the hidden layer goes to infinity.\n",
    "\n",
    "there are also some disadvantages\n",
    "\n",
    "* Many non-linearity won't work on the output layer (has to be invertable).\n",
    "* You're limited to only one hidden layer, which typically has to be very wide — multiple layers has a combinatoric effect, that mean you get away with less units and that's not something that can be exploited here.\n",
    "* They like to overfit, but the HyperOpt should minimise that. In fact, this weakness makes them an ideal candidate for trying out HyperOpt!\n",
    "\n",
    "Your first task is to implement this algortihm. The trainning subsection below describes it step-by-step; don't forget you'll need a predict method as well.\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Randomly generate a weight matrix for the first layer. There is techneque to this: you want to pick the weights going into each unit to be vectors with the direction evenly distributed but the length uniformly distributed in a range that you specify with a pair of hyperparameters (low and high). This is done in three steps\n",
    "    1. Fill the weight matrix with draws from a standard Gaussian.\n",
    "    2. Normalise all weights for a single hidden unit so it's weight vector is of length 1 — this results in unit length vectors evenly distributed on the surface of the unit hyper-sphere.\n",
    "    3. Scale each hidden units weight vector by a draw from a uniform distribution, covering the required range.\n",
    "2. Generate a \"new\" set of input features, by running the original input features through the weight matrix followed by applying the non-linearity, which will be `numpy.sin()`. Try and avoid looping here, as that will be very slow.\n",
    "3. Concatenate a one to the end of the feature vector — this will act as the bias term in the following step.\n",
    "4. You now have a linear regression problem; solve it to generate the weights that connect the hidden layer to the output (it will be a vector because there is only one output).\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "* A framework class has been provided; you may choose to use it or bin it and replace it with your own. But it might help to already have a structure to work with.\n",
    "* No need to use `PyTorch` or equivalent — `numpy` is enough!\n",
    "* Make sure the code broadcasts as much as is reasonable: model answer runs in under a second on one particular computer.\n",
    "* To generate random numbers you will want to create a random number generator with something like `rng = numpy.random.default_rng(0)` (0 fixes the seed, so it produces the same sequence of \"random\" numbers every time) and then call the many methods it provides for generation, e.g. `rng.standard_normal()` if you want a draw from the standard Gaussian (it takes a `size` parameter if you want many such values).\n",
    "* Double check you've normalised the correct dimension when generating the weights: get it wrong and it will destroy performance. Most of the other mistakes will make it crash instead:-)\n",
    "* `numpy.linalg.lstsq()` solves linear equations!\n",
    "* While `numpy.einsum()` may cause some existential terror, it's very useful and worth learning. For instance, `numpy.einsum('rc,...c->...r', a, x)` does a matrix multiplication on every vector in x, even if x contains many vectors; this is what each layer of a NN is doing. Yes, the same thing can be done with `numpy.tensordot()`, but this is arguably clearer and more general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomKitchenSink:\n",
    "    \"\"\"Regresses a single number given an input feature vector.\"\"\"\n",
    "    \n",
    "    def __init__(self, low, high, input_width = 11, hidden_width = 512):\n",
    "        \"\"\"Initialises the model with the random vector length (low to high) of the\n",
    "        random weights connecting the input to the hidden layer, plus how many input\n",
    "        units there are and how many hidden units.\"\"\"\n",
    "        self._low = low\n",
    "        self._high = high\n",
    "        self._input_width = input_width\n",
    "        self._hidden_width = hidden_width\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Fits the model given x (a data matrix, indexed [exemplar, feature])\n",
    "        and y (a vector, indexed [exemplar]).\"\"\"\n",
    "        \n",
    "        # Generate random weight matrix between input and hidden layer\n",
    "\n",
    "        # Send the x values through the first half of the NN\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Given x predicts y. Only needs to accept data matrices indexed [exemplar, feature].\"\"\"\n",
    "        \n",
    "        # Code me!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Obviously, you want to test an algorithm before taking it further — the below runs on the training set and calculates the RMSE on the test set with `low` set to `0` and `high` set to `1`, with `512` hidden units. If you've changed the framework you may need to adapt the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_x, train_y)\n\u001b[1;32m      4\u001b[0m train_pred_y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(train_x)\n\u001b[0;32m----> 5\u001b[0m test_rmse \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39msqrt(numpy\u001b[38;5;241m.\u001b[39msquare(\u001b[43mtrain_pred_y\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain root mean square error = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_rmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m test_pred_y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_x)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "model = RandomKitchenSink(0, 1)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "train_pred_y = model.predict(train_x)\n",
    "test_rmse = numpy.sqrt(numpy.square(train_pred_y - train_y).mean())\n",
    "print(f'Train root mean square error = {test_rmse}')\n",
    "\n",
    "test_pred_y = model.predict(test_x)\n",
    "test_rmse = numpy.sqrt(numpy.square(test_pred_y - test_y).mean())\n",
    "print(f'Test root mean square error = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HyperOpt\n",
    "\n",
    "Now optimise the two hyperparameters, low and high! That really is the extent of your instructions; there is an assumption you'll try a genetic algorithm but feel free to try anything suitable from the optimisation lecture. If you find yourself in a group aim to all try different variations. Keep in mind that `low` and `high` have an ordering constraint and should be positive; you may want to consider an alternate representation so you don't have to enforce these constraints.\n",
    "\n",
    "Steps:\n",
    "1. Write a function that takes low and high as parameters and returns a measure of how well they work, by training the algoroithm on the training set and then using the validation set to measure performance for the HyperOpt.\n",
    "2. Write a function that generates random pairs of low/high, or whatever representation is used; this is used to initialise the population of solutions and implicitely defines the search space. If you're unsure of what a sensible search space is then try some numbers out in the above testing code to get a feel for what's plausible.\n",
    "3. Write an optimisation loop that does the steps of a genetic algorithm: hill climbing, mutation and selection.\n",
    "4. Include reporting (aka `print()`) so you can see what it's doing as it runs.\n",
    "5. After the loop has ended report the train, validation and test performance: if it has worked then these should be extremely close to each other, as the point is to avoid the gap that indicates over/under fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code me!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
