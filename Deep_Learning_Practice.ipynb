{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([0.09291784,0.46809093,0.93089486,0.67612654,0.73441752,0.86847339,\\\n",
    "                   0.49873225,0.51083168,0.18343972,0.99380898,0.27840809,0.38028817,\\\n",
    "                   0.12055708,0.56715537,0.92005746,0.77072270,0.85278176,0.05315950,\\\n",
    "                   0.87168699,0.58858043])\n",
    "y_train = np.array([-0.25934537,0.18195445,0.651270150,0.13921448,0.09366691,0.30567674,\\\n",
    "                    0.372291170,0.20716968,-0.08131792,0.51187806,0.16943738,0.3994327,\\\n",
    "                    0.019062570,0.55820410,0.452564960,-0.1183121,0.02957665,-1.24354444, \\\n",
    "                    0.248038840,0.26824970])\n",
    "y_train_binary = np.array([0,1,1,0,0,1,\\\n",
    "                    1,0,0,1,0,1,\\\n",
    "                    0,1,1,0,1,0, \\\n",
    "                    1,1])\n",
    "y_train_multiclass = np.array([2,0,1,2,1,0,\\\n",
    "                    0,2,2,0,2,0,\\\n",
    "                    2,0,1,2,1,2, \\\n",
    "                    1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Rectified Linear Unit (ReLU) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(preactivation):\n",
    "    return np.clip(preactivation,a_min=0,a_max=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function that maps [-infty,infty] to [0,1]\n",
    "def sigmoid(model_out):\n",
    "  sig_model_out = 1/(1+np.exp(-model_out))\n",
    "  return sig_model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function that maps a vector of arbitrary values to a vector of values that are positive and sum to one.\n",
    "def softmax(model_out):\n",
    "  # Compute the exponential of the model outputs\n",
    "  exp_model_out = np.exp(model_out)\n",
    "  # Compute the sum of the exponentials (denominator of equation 5.22)\n",
    "  sum_exp_model_out = np.sum(exp_model_out, axis =0)\n",
    "  # Normalize the exponentials (np.matlib.repmat might be useful here)\n",
    "  softmax_model_out = exp_model_out/np.matlib.repmat(sum_exp_model_out, exp_model_out.shape[0], 1)\n",
    "\n",
    "  return softmax_model_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a shallow neural network with two inputs, two outputs, and three hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_2_2_3(x1,x2, activation_fn, phi_10, phi_11, phi_12, phi_13, phi_20, phi_21, phi_22, phi_23, theta_10, theta_11, theta_12, theta_20, theta_21, theta_22, theta_30, theta_31, theta_32):\n",
    "    h1 = activation_fn(theta_10 + theta_11*x1 + theta_12*x2)\n",
    "    h2 = activation_fn(theta_20 + theta_21*x1 + theta_22*x2)\n",
    "    h3 = activation_fn(theta_30 + theta_31*x1 + theta_32*x2)\n",
    "\n",
    "    y1 = phi_10 + phi_11*h1 + phi_12*h2 + phi_13*h3\n",
    "    y2 = phi_20 + phi_21*h1 + phi_22*h2 + phi_23*h3\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallow Neural Network in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_1_1_1(x, beta_0, omega_0, beta_1, omaga_1):\n",
    "    n_data = x.size\n",
    "    x = np.reshape(x,(1,n_data))\n",
    "    h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(omega_0,x))\n",
    "    y = np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(omega_1,h1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the maximum possible number of linear regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_regions(Di, D):\n",
    "  # TODO -- implement Zaslavsky's formula\n",
    "  # You will need to use math.factorial() https://www.geeksforgeeks.org/factorial-in-python/\n",
    "  # Replace this code\n",
    "  N = 0\n",
    "  for j in range(0, Di+1):\n",
    "    N += factorial(D)/(factorial(D - j)* factorial(j))\n",
    "\n",
    "  return N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a deep neural network with, one input, one output, two hidden layers and three hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sizes\n",
    "D_i=4; D_1=5; D_2=2; D_3=1; D_o=1\n",
    "# We'll choose the inputs and parameters of this network randomly using np.random.normal\n",
    "# For example, we'll set the input using\n",
    "n_data = 4;\n",
    "x = np.random.normal(size=(D_i, n_data))\n",
    "# TODO initialize the parameters randomly with the correct sizes\n",
    "# Replace the lines below\n",
    "beta_0 = np.random.normal(size=(D_1,1))\n",
    "Omega_0 = np.random.normal(size=(D_1,D_i))\n",
    "beta_1 = np.random.normal(size=(D_2,1))\n",
    "Omega_1 = np.random.normal(size=(D_2,D_1))\n",
    "beta_2 = np.random.normal(size=(D_3,1))\n",
    "Omega_2 = np.random.normal(size=(D_3,D_2))\n",
    "beta_3 = np.random.normal(size=(D_o,1))\n",
    "Omega_3 = np.random.normal(size=(D_o,D_3))\n",
    "\n",
    "\n",
    "# If you set the parameters to the correct sizes, the following code will run\n",
    "h1 = ReLU(np.matmul(beta_0,np.ones((1,n_data))) + np.matmul(Omega_0,x));\n",
    "h2 = ReLU(np.matmul(beta_1,np.ones((1,n_data))) + np.matmul(Omega_1,h1));\n",
    "h3 = ReLU(np.matmul(beta_2,np.ones((1,n_data))) + np.matmul(Omega_2,h2));\n",
    "y = np.matmul(beta_3,np.ones((1,n_data))) + np.matmul(Omega_3,h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert neural network in matrix form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum_of_squares(y_train, y_pred):\n",
    "    sum_of_squares = np.sum((y_train-y_pred) * (y_train-y_pred))\n",
    "    return sum_of_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = np.zeros((3,1));  # formerly theta_x0\n",
    "omega_0 = np.zeros((3,1)); # formerly theta_x1\n",
    "beta_1 = np.zeros((1,1));  # formerly phi_0\n",
    "omega_1 = np.zeros((1,3)); # formerly phi_x\n",
    "\n",
    "beta_0[0,0] = 0.3; beta_0[1,0] = -1.0; beta_0[2,0] = -0.5\n",
    "omega_0[0,0] = -1.0; omega_0[1,0] = 1.8; omega_0[2,0] = 0.65\n",
    "beta_1[0,0] = 0.1;\n",
    "omega_1[0,0] = -2.0; omega_1[0,1] = -1.0; omega_1[0,2] = 7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univeriate Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return probability under normal distribution for input x\n",
    "def normal_distribution(y, mu, sigma):\n",
    "    prob = (1/np.sqrt(2*math.pi*sigma*sigma))* np.exp(-((y-mu) * (y-mu)/(2*sigma*sigma)))\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihood(y_train, mu, sigma):\n",
    "    likelihood = np.prod(normal_distribution(y_train, mu, sigma))\n",
    "    return likelihood\n",
    "def compute_negative_log_likelihood(y_train, mu, sigma):\n",
    "    nll = -1 * np.sum(np.log(normal_distribution(y_train, mu, sigma)))\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_pred = shallow_1_1_1(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "sigma = 0.2\n",
    "likelihood = compute_likelihood(y_train, mu_pred, sigma)\n",
    "nll = compute_negative_log_likelihood(y_train, mu_pred, sigma)\n",
    "sum_of_squares = compute_sum_of_squares(y_train, mu_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum of the likelihood fn is at the same position as the minimum negative log likelihood and the least squares solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return probability under Bernoulli distribution for input x\n",
    "def bernoulli_distribution(y, lambda_param):\n",
    "    prob = np.power(1-lambda_param,(1-y)) * np.power(lambda_param,y)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, lambda_param):\n",
    "  prob = bernoulli_distribution(y_train, lambda_param)\n",
    "  likelihood = np.prod(prob)\n",
    "  return likelihood\n",
    "\n",
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, lambda_param):\n",
    "  nll = -1 * np.sum(np.log(bernoulli_distribution(y_train, lambda_param)))\n",
    "  return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out = shallow_1_1_1(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "lambda_train = sigmoid(model_out)\n",
    "likelihood = compute_likelihood(y_train_binary, lambda_train)\n",
    "nll = compute_negative_log_likelihood(y_train_binary, lambda_train)\n",
    "sum_of_squares = compute_sum_of_squares(y_train_binary, model_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return probability under Categorical distribution for input x\n",
    "# Just take value from row k of lambda param where y =k,\n",
    "def categorical_distribution(y, lambda_param):\n",
    "    return np.array([lambda_param[row, i] for i, row in enumerate (y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Pass the outputs through the softmax function\u001b[39;00m\n\u001b[1;32m     13\u001b[0m lambda_train \u001b[38;5;241m=\u001b[39m softmax(model_out)\n\u001b[0;32m---> 14\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m compute_likelihood(y_train_multiclass, lambda_train)\n\u001b[1;32m     15\u001b[0m nll \u001b[38;5;241m=\u001b[39m compute_negative_log_likelihood(y_train_multiclass, lambda_train)\n",
      "Cell \u001b[0;32mIn[114], line 3\u001b[0m, in \u001b[0;36mcompute_likelihood\u001b[0;34m(y_train, lambda_param)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_likelihood\u001b[39m(y_train, lambda_param):\n\u001b[0;32m----> 3\u001b[0m   likelihood \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(categorical_distribution(y_train, lambda_param))\n\u001b[1;32m      4\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m likelihood\n",
      "Cell \u001b[0;32mIn[113], line 4\u001b[0m, in \u001b[0;36mcategorical_distribution\u001b[0;34m(y, lambda_param)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcategorical_distribution\u001b[39m(y, lambda_param):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([lambda_param[row, i] \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m (y)])\n",
      "Cell \u001b[0;32mIn[113], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcategorical_distribution\u001b[39m(y, lambda_param):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([lambda_param[row, i] \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m (y)])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Return the likelihood of all of the data under the model\n",
    "def compute_likelihood(y_train, lambda_param):\n",
    "  likelihood = np.prod(categorical_distribution(y_train, lambda_param))\n",
    "  return likelihood\n",
    "\n",
    "# Return the negative log likelihood of the data under the model\n",
    "def compute_negative_log_likelihood(y_train, lambda_param):\n",
    "  nll = -1 * np.sum(np.log(categorical_distribution(y_train, lambda_param)))\n",
    "  return nll\n",
    "\n",
    "model_out = shallow_1_1_1(x_train, beta_0, omega_0, beta_1, omega_1)\n",
    "# Pass the outputs through the softmax function\n",
    "lambda_train = softmax(model_out)\n",
    "likelihood = compute_likelihood(y_train_multiclass, lambda_train)\n",
    "nll = compute_negative_log_likelihood(y_train_multiclass, lambda_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
