{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uyR6ZVy9dLuEt_SVff2eeyIN39uP1RLO","timestamp":1696352115090},{"file_id":"1boKkFTCnnQV_ynMoO1VVmui-SpuXpfvS","timestamp":1696059358548},{"file_id":"1KcufXbaZ8CKUDGUrAlAPhUXyXu4f61D6","timestamp":1696010330567},{"file_id":"1eGLYS_zUuxA5bM0xNMOzJH865KP6ob4d","timestamp":1695989055217},{"file_id":"1r66ATmzuj733WGiT0d1U2UelyFJInFfq","timestamp":1690925588662},{"file_id":"1X5kOyD70x-esRx9RnrIeLmZy5FddAvPn","timestamp":1670251197383}],"authorship_tag":"ABX9TyNxMM5Pcd/xdZ/j2miu2TTo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Notebook 13.4: Graph attention networks**\n","\n","This notebook builds a graph attention mechanism from scratch, as discussed in section 13.8.6 of the book and illustrated in figure 13.12c\n","\n","Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n","\n","Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n","\n"],"metadata":{"id":"t9vk9Elugvmi"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"OLComQyvCIJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n","\n"],"metadata":{"id":"9OJkkoNqCVK2"}},{"cell_type":"code","source":["# Set seed so we get the same random numbers\n","np.random.seed(1)\n","# Number of nodes in the graph\n","N = 8\n","# Number of dimensions of each input\n","D = 4\n","\n","# Define a graph\n","A = np.array([[0,1,0,1,0,0,0,0],\n","              [1,0,1,1,1,0,0,0],\n","              [0,1,0,0,1,0,0,0],\n","              [1,1,0,0,1,0,0,0],\n","              [0,1,1,1,0,1,0,1],\n","              [0,0,0,0,1,0,1,1],\n","              [0,0,0,0,0,1,0,0],\n","              [0,0,0,0,1,1,0,0]]);\n","print(A)\n","\n","# Let's also define some random data\n","X = np.random.normal(size=(D,N))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAygJwLiCSri","executionInfo":{"status":"ok","timestamp":1696355361993,"user_tz":-60,"elapsed":391,"user":{"displayName":"Simon Prince","userId":"08883977293436534727"}},"outputId":"ba26b07e-9f3d-474a-c224-4e285ce755fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 1 0 1 0 0 0 0]\n"," [1 0 1 1 1 0 0 0]\n"," [0 1 0 0 1 0 0 0]\n"," [1 1 0 0 1 0 0 0]\n"," [0 1 1 1 0 1 0 1]\n"," [0 0 0 0 1 0 1 1]\n"," [0 0 0 0 0 1 0 0]\n"," [0 0 0 0 1 1 0 0]]\n"]}]},{"cell_type":"markdown","source":["We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"],"metadata":{"id":"W2iHFbtKMaDp"}},{"cell_type":"code","source":["# Choose random values for the parameters\n","omega = np.random.normal(size=(D,D))\n","beta = np.random.normal(size=(D,1))\n","phi = np.random.normal(size=(1,2*D))"],"metadata":{"id":"79TSK7oLMobe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll need a softmax operation that operates on the columns of the matrix and a ReLU function as well"],"metadata":{"id":"iYPf6c4MhCgq"}},{"cell_type":"code","source":["# Define softmax operation that works independently on each column\n","def softmax_cols(data_in):\n","  # Exponentiate all of the values\n","  exp_values = np.exp(data_in) ;\n","  # Sum over columns\n","  denom = np.sum(exp_values, axis = 0);\n","  # Replicate denominator to N rows\n","  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n","  # Compute softmax\n","  softmax = exp_values / denom\n","  # return the answer\n","  return softmax\n","\n","\n","# Define the Rectified Linear Unit (ReLU) function\n","def ReLU(preactivation):\n","  activation = preactivation.clip(0.0)\n","  return activation\n"],"metadata":{"id":"obaQBdUAMXXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # Now let's compute self attention in matrix form\n","def graph_attention(X,omega, beta, phi, A):\n","\n","  # TODO -- Write this function (see figure 13.12c)\n","  # 1. Compute X_prime\n","  # 2. Compute S\n","  # 3. To apply the mask, set S to a very large negative number (e.g. -1e20) everywhere where A+I is zero\n","  # 4. Run the softmax function to compute the attention values\n","  # 5. Postmultiply X' by the attention values\n","  # 6. Apply the ReLU function\n","  # Replace this line:\n","  output = np.ones_like(X) ;\n","\n","  # BEGIN_ANSWER\n","  X_prime = beta * np.ones((1,N)) + omega @ X\n","  S = np.zeros((N,N))\n","  for c1 in range(N):\n","    for c2 in range(N):\n","      S[c1,c2] = phi @ (np.vstack([X[:,[c1]],X[:,[c2]]]))\n","\n","  # print(S)\n","  mask = (A+np.identity(N) !=1) * np.ones((N,N))\n","  S = S - mask * S\n","  S = S - mask * 1e20\n","  attention = softmax_cols(S)\n","  output = ReLU(X_prime @ attention)\n","\n","  # END_ANSWER\n","  return output;"],"metadata":{"id":"gb2WvQ3SiH8r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test out the graph attention mechanism\n","np.set_printoptions(precision=3)\n","output = graph_attention(X, omega, beta, phi, A);\n","print(\"Correct answer is:\")\n","print(\"[[1.796 1.346 0.569 1.703 1.298 1.224 1.24  1.234]\")\n","print(\" [0.768 0.672 0.    0.529 3.841 4.749 5.376 4.761]\")\n","print(\" [0.305 0.129 0.    0.341 0.785 1.014 1.113 1.024]\")\n","print(\" [0.    0.    0.    0.    0.35  0.864 1.098 0.871]]]\")\n","\n","\n","print(\"Your answer is:\")\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4p6HyHXmDh5","executionInfo":{"status":"ok","timestamp":1696355447729,"user_tz":-60,"elapsed":240,"user":{"displayName":"Simon Prince","userId":"08883977293436534727"}},"outputId":"1cd80f6b-25b6-483b-9c22-9276d971fc7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correct answer is:\n","[[1.796 1.346 0.569 1.703 1.298 1.224 1.24  1.234]\n"," [0.768 0.672 0.    0.529 3.841 4.749 5.376 4.761]\n"," [0.305 0.129 0.    0.341 0.785 1.014 1.113 1.024]\n"," [0.    0.    0.    0.    0.35  0.864 1.098 0.871]]]\n","Your answer is:\n","[[1.796 1.346 0.569 1.703 1.298 1.224 1.24  1.234]\n"," [0.768 0.672 0.    0.529 3.841 4.749 5.376 4.761]\n"," [0.305 0.129 0.    0.341 0.785 1.014 1.113 1.024]\n"," [0.    0.    0.    0.    0.35  0.864 1.098 0.871]]\n"]}]},{"cell_type":"markdown","source":["TODO -- Try to construct a dot-product self-attention mechanism as in practical 12.1 that respects the geometry of the graph and has zero attention between non-neighboring nodes by combining figures 13.12a and 13.12b.\n","\n","ANSWER: The self-attention mechanisms works as normal but we set all values of the pre-softmax attention to 1e-20 where A+I is zero.\n"],"metadata":{"id":"QDEkIrcgrql-"}}]}